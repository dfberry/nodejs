import { TextLoader } from 'langchain/document_loaders/fs/text'
import { DirectoryLoader } from 'langchain/document_loaders/fs/directory'
import { OpenAIEmbeddings } from '@langchain/openai'
import { Document } from 'langchain/document'
import { CharacterTextSplitter } from '@langchain/textsplitters'
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import { ChatOpenAI } from '@langchain/openai'
import { MemoryVectorStore } from 'langchain/vectorstores/memory'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { pull } from 'langchain/hub'
import { Annotation, StateGraph } from '@langchain/langgraph'
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import "cheerio";

import 'dotenv/config'
import { get } from 'http'

async function loadDocs() {
  const loader = new DirectoryLoader('src/docs/typespec_shallow', {
    '.md': (path) => new TextLoader(path),
  })
  const docs: Document<Record<string, any>>[] = await loader.load()
  return docs
}
async function splitDocs(docs: any) {
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 100,
  })

  return splitter.splitDocuments(docs)
}
async function indexChunks(vectorStore: any, docsSplit: any) {
  await vectorStore.addDocuments(docsSplit)
}
async function embedDocs(embeddings: any, docsOnly: string[]) {
  const docsEmbeddings = await embeddings.embedDocuments(docsOnly)
  return docsEmbeddings
}
async function getPromptTemplate() {
  const promptTemplate = await pull<ChatPromptTemplate>('rlm/rag-prompt')
  return promptTemplate
}



// Load and chunk contents of blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000, chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);


// Index chunks
await vectorStore.addDocuments(allSplits)

// Define prompt for question-answering
const promptTemplate = await pull<ChatPromptTemplate>("rlm/rag-prompt");

// Define state for application
const InputStateAnnotation = Annotation.Root({
  question: Annotation<string>,
});

const StateAnnotation = Annotation.Root({
  question: Annotation<string>,
  context: Annotation<Document[]>,
  answer: Annotation<string>,
});

// Define application steps
const retrieve = async (state: typeof InputStateAnnotation.State) => {
  const retrievedDocs = await vectorStore.similaritySearch(state.question)
  return { context: retrievedDocs };
};


const generate = async (state: typeof StateAnnotation.State) => {
  const docsContent = state.context.map(doc => doc.pageContent).join("\n");
  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });
  const response = await llm.invoke(messages);
  return { answer: response.content };
};


// Compile application and test
const graph = new StateGraph(StateAnnotation)
  .addNode("retrieve", retrieve)
  .addNode("generate", generate)
  .addEdge("__start__", "retrieve")
  .addEdge("retrieve", "generate")
  .addEdge("generate", "__end__")
  .compile();